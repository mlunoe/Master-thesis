%!TEX root = thesis.tex
\chapter{Evaluation of the Solution} % (fold)
\label{ch:evaluation}
%- Evaluate the solution and compare existing techniques for personalisation to the ones used in the solution.

Evaluate using the in \cite{Sections-categories-and-keywords-as-interest-specification-tools-for-personalised-news-services.pdf} and \cite{Evaluating-a-User-Model-Based-Personalisation-Architecture-for-Digital-News-Services.pdf} presented method. Their categorisation based on few keywords (5 as the lowest) to represent a category resulted in poor evaluation, this gives a good motivation for including more keywords and using Wordnet to enrich the set of keywords.

It could be interesting to evaluate the precision and recall points based on: news items per section, news items per category, maximum number of news items per message required by the user, general relevance of the contents of a given day for a given user, etc. as \cite{Sections-categories-and-keywords-as-interest-specification-tools-for-personalised-news-services.pdf} proposes.

\cite{Sections-categories-and-keywords-as-interest-specification-tools-for-personalised-news-services.pdf} also raise the problem of precision in finding news within based on a single day. This can hopefully be handled by having the user specify in which period of time he wants news and maybe notify the user that solutions might be inaccurate if a limited period of time is chosen, or just limit the user to specify 24 hours as a minimal value.
%``just as newspaper must come full of news everyday, regardless of whether anything interesting has happened or not, newspaper sections will carry a certain amount of news independently of their relevance to the section heading, and a personalised news message will feature some news item distantly related to the user profile.''


\subsection{Initial Test}
\todo[inline]{NOT ONLY 5 test persons \protect\cite{NielsenTest}. This has been discarded by many.}
%\cite{NielsenTest} says that 5 test persons in a qualitative test is enough to uncover most preferences and issues, but the test included 7 test persons to be more adequate.

\subsection{Result}

\section{Test}

\subsection{Layout}

classify items as first and second articles and use layout to distinguish them.

columns constraints, white space and odd placement of articles.

We follow a strict vertical structure, but there is a lot of work to be done with the horizontal structure

In order to come closer to understanding what newspapers does it could be interesting to analyse their component structure, e.g.\ using \cite{00953970.pdf} algorithm.

At an early stage the paging of a section was discarded, because a preliminary test (ask around) showed that users wanted to scroll down to see the full section. This was also necessary if a full-length articles were to be shown, but the reading behaviour analysis suggests that articles should be divided into chunks of subjects, which may be better to visualise using pages. This way the featured article could be shown in a longer length excerpt and stories on the same subject could surround it with only headlines, images and short excerpts shown. If the user then wants to read one of the articles in full length he can select it and the full article could be displayed, using the full size of the screen.%Non-featured article he could select it, showing interest in it, and the system could generate a new page where this article is featured. The article should then again be surrounded by new non-featured articles, that of cause are not placed anywhere else in the paper.

\subsection{Content}
The system can be used for automatic classification of articles. Of cause, then a sufficient list of categories and their definitions must be used. This can either be retrieved by the list of Google News categories\footnote{\url{http://support.google.com/webmasters/bin/answer.py?hl=en&answer=42993}.} and a Wordnet enriched list of key words from Google News list of suggested keywords\footnote{\url{http://support.google.com/news/publisher/bin/answer.py?hl=en&answer=116037}.} or by the root terms presented in \cite{10-1-1-19-5583}. These can later on be refined by information retrieved by the user behaviour in the system and manually removal of false negatives? However, also more advanced techniques of text classification could be used in later stages of the system, like one presented in \cite{Evaluating-a-User-Model-Based-Personalisation-Architecture-for-Digital-News-Services.pdf}.

\todo[inline]{Maybe find a better example of text classification.}

Use of automatic generation of personal item summaries \cite{fulltext.pdf}


Use geotargeting to supply local news.

Use a thesaurus and predefined root terms as in \cite{10-1-1-19-5583} which improves classification; semantic knowledge is more general than keywords.

scattered ads \cite{kristin_fredrik.pdf}

\subsection{Functionality}
Order a print copy of the newspaper



The development of the Internet from a distributor of information to a library of digital applications has deeply integrated the users in every step of an applications lifetime. It has become harder to distinguish between super users and developers, applications are branched and modified according to every need and authors can therefore no longer predict which use his application can be to another user -- nor should he have to.

\subsection{Improvements}
\cite{Personalizing-your-electronic-newspaper.pdf} suggest virtual communities, or individuals with common interests.

Weights on key words should be adjusted by a strength (or an uncertainty) of prediction as it is proposed in \cite{10.1.1.45.5230.pdf}.

Count the number of sources have included an articles that are very similar to find the breaking factor.

How to handle that a user is not presented with an already read article? \cite{User-Modeling-for-Adaptive-News-Access.pdf} presents the nearest neighbour (NN) algorithm approach, using a tf-idf similarity, to determine whether the story is already known, i.e.\ the similarity to the NN read story is above a given threshold. This could be solved by keeping a library of read items (this can be done along with the tracking of which article is in focus) and then match new items against this banned list and down prioritise them if their similarity is too high. This could be done with the same polynomial function as used between articles.

``Distinguishing between short-term and long-term models has several desirable qualities in domains with temporal characteristics (Chiu and Webb, 1998).'' \cite{User-Modeling-for-Adaptive-News-Access.pdf}.

\todo[inline]{What things were even better or a little worse than expected regarding the methods you used to solve your problems. How could your project be improved by further work.}

% section evaluation (end)