%!TEX root = ../thesis.tex
\chapter{Related work}
\label{ch:related_work}
This chapter will look at the explored literature to find inspiration for the solution to be implemented. It discusses relevant potential solutions and concludes with a choice of approach.

Web personalisation is by \cite{DataMiningMobasher} divided into phases of data collection and preprocessing, pattern discovery and evaluation, and applying the discovered knowledge in real-time to mediate between the user and the Web. There have been many suggestions on how to tackle these different processes of creating the interactive personalised digital newspaper. \cite{gervasum2001ws.pdf} proposes a strictly probabilistic approach to dynamic personalisation obtained by characterisation of content and user's interests. Both implicit and explicit relevance feedback\sidenote{Implicit is when the (unaware) user's behaviour is recorded to determine relevance, and explicit is when the user is aware of the action of giving the feedback.} is used to refine the user models. Probabilistic approaches have the advantage of being effective, but often solves a very specific problem. Also, these approaches tend to get very complex in order to deliver promising results. Some cope with this by introducing logic to the problem like it is done in \cite{SpacialLogicNilsson} with a spatial approach. In this project it is possible to benefit from the structure of the logic approach of CP and the effectiveness of a probabilistic approach by introducing preference constraints with an objective function.

Many use the approach of computing the TF-IDF similarity. TF-IDF is weighting of words in a document represented by a Vector Space Model (VSM) as described in \cite{a-vector-space-model-for-automatic-indexing.pdf}. When documents have been represented by VSM, their similarity can be determined using a cosine angle between them to compute a fast result. The method has been used in \cite{fulltext.pdf} to apply relational personalisation, where a set of keywords has been extracted from the news items to produce promising results, based on training with relevant documents. The use of TF-IDF constitutes the initial approach for computing similarity in this project. Later on a keyword based approach was implemented, which constitutes the final solution.
%\todo[inline]{Beskrive fordele ved TF-IDF. \protect\cite{a-vector-space-model-for-automatic-indexing.pdf} and \protect\shortcite{Evaluating-a-User-Model-Based-Personalisation-Architecture-for-Digital-News-Services.pdf}}

Classification techniques can also be applied in order to ease the task of selecting relevant articles and determine their mutual relationships. \cite{gervasum2001ws.pdf} uses a library of documents to train a categorisation algorithm, and the users are then asked to select categories of which they have interest. \cite{10-1-1-19-5583}, on the other hand uses a thesaurus of hierarchically, and to the task specifically, structured terms to index news articles. Results of the indexing are thereafter mapped with user profiles to select the relevant articles. In the paper, \cite{10-1-1-19-5583} argues that semantic knowledge is more substantial than keywords. However, instead of using predefined root terms as the basis for a classification, WordNet can be used to obtain semantic knowledge for a document. WordNet\sidenote{See \url{http://wordnet.princeton.edu}.} is a large lexical database of English words and their relationships in the form of different graphs. \cite{116262780379.pdf} presents an algorithm for enriching articles using WordNet's hypernym-graphs. WordNet also contains similarity functions between words. These functions could be used to solve the problem proposed in this paper. %later on constitute the next step for computing similarity in this project.

\cite{fulltext.pdf} does, however, present the means of combining the use of categories and keywords, but this approach demands predefined categories, which must be kept updated in order to follow semantic changes to the field. The time limitations and prioritisation of this project did not allow for a thesaurus to be obtained to aid the classification and will therefore not be introduced to the solution. One, could also argue that semantic assumptions are made, when categories are predefined, which could lead to some imprecise classification. \cite{116262780379.pdf}'s algorithm, on the other hand, is based only on words from the article and the general semantic (and more neutral) structure that constitutes the basis for WordNet.
%\cite{10.1.1.45.5230.pdf} presents a combination of content-based and collaborate filters to predict interest in articles based on a user profile.
%
%\cite{10.1.1.45.5230.pdf} presents a front page design and available sections. A section can appear as their front page.
%relevance feedback
%
%their editorial mix consists of ordering the articles by most predicted interest first.

To represent the users' interests, different approaches have been explored. The most promising results are generated by a short- and long-term representation of the user model as presented by \cite{fulltext.pdf} and \cite{DataMiningMobasher}. \cite{fulltext.pdf} also propose a global user profile, to get the process of generating the user model started. It seems that this would be a viable approach.

%\cite{Personalizing-your-electronic-newspaper.pdf} incorporates temporal personalisation in that it is possible to ask for articles in the newspaper based on a specific period, but also by incorporate ageing of user interests.
%
%\shortcite{Estebanetal.pdf} incorporates temporal features of their personalisation of a digital newspaper based on Yahoo! Spain. Automatic Categorisation of news items, long- and short-term user models.
%
%In the explored literature users shows much interest in being able to turn pages as it is done in a regular newspaper. \cite{FULLTEXT01.pdf} describes this as ``open, turn pages, chose article, read and return''.
\cite{fulltext.pdf} propose the use of collaborative filtering\sidenote{Making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).} to handle the problem of converging, which is what will happen if no non-personalised articles are introduced. Collaborative filtering, however, still only concerns articles that are within the area of the user's interest. If e.g.\ a user has not shown interest in politics, the news of Barack Obama becoming the President of USA will never be included in the newspaper. Instead a ratio between personalised and general articles will solve this issue, and since it is not within everyones interest to receive general news, this ratio should be adjustable.
%Users navigate the newspaper using sections and headlines as the main entry points \cite{FULLTEXT01.pdf} and these should therefore be kept in the digital version.
%
%Users express that these should be put into menu \cite{kristin-fredrik.pdf}.
%
%\cite{fulltext.pdf} proposes personalised excerpts from the articles to further ease the navigation.

There exist many different examples of preference modelling using CP, and \cite{Constraint-Satisfaction-Methods-for-Information-Personalization.pdf} describe a factual information system to find personal information, e.g.\ about healthcare. They present two constraints: (1) select only information-objects that correspond to the user-model and; (2) the content of the retained information-items do not contradict each other. This is an example of an editorial mix in that it incorporates the relational features, i.e.\ both between user and articles, and articles in between. However, they do not take into account the spatial part of their editorial mix, nor do they take into account any temporal features of the information needed. It is of cause notable that the user needs for a strictly factual information system are different than from a newspaper.

Another application of preference modelling using CP is \cite{LSVossen}. He proposes a CP approach to automatic playlist generation, which very much relates to what this project attempts to achieve. A playlist can, in this context, be perceived as a personal mix of songs. He presents constraints to exclude songs with certain attributes and constraints to model that certain songs should be similar to each other or a user preference. He also presents constraints to describe preference about the number of songs from a specific artist and finally, the well-known \texttt{all-diff} constraint. These can be directly translated to the editorial mix of a newspaper, where the songs are articles and an artist could be a specific author or content provider.
\clearpage
The presented relevant features can be summed up in the following list:
\begin{itemize}\itemdist
	\item fast computation using TF-IDF
	\item extension of similarity computation using a thesaurus
	\item WordNet enrichment of articles and extraction of keywords
	\item both a long-term and short-term representation of the user model
	\item collaborative filtering
	\item personal and general news in combination
	\item CP has been introduced to solve similar personalisation problems
\end{itemize}

This paper proposes a Constraint Programming (CP) approach to personalise the editorial mix. The problem will be expressed as a Constraint Optimisation Problem and solved using local search for CP taking advantage of both probabilistic and logical approaches. Furthermore, this paper proposes a keyword based solution, using WordNet enrichment of articles in combination with a comparison of entities, to determine the relevance of an article to a user defined topic and similarity between articles. A representation of the user model will not be chosen, as the focus lies with the composition of the newspaper. Instead, the representation of user needs will be defined manually to base the application on and make it ready for the implementation of the user model.

%\todo[inline]{Er der argumenteret for at bruge keywords?}
%personalised topics instead of the generic definition of the terms. Maybe what the crowd thinks is technology is not what you deem as technology

%``Constraint programming (CP) is an emergent software technology for declarative description and effective solving of large, particularly combinatorial, problems especially in areas of planning and scheduling. [...] it is attracting widespread commercial interest as well, in particular, in areas of modelling heterogeneous optimisation and satisfaction problems.'' \cite{RomanCP}
%
%Where CP provides a declarative approach, with the possibility of introducing stochastic variables a purely stochastic approach also has it advantages.
%
%Stochastic approaches like the graph based Hidden Markov Model or the spatial like the one proposed in  are usually very effective, but often at the cost of being imprecise, but logic can also be introduced to stochastic approaches, \cite{SpacialNilsson}.
%\todo[inline]{Find reference for this.}
%
%A graph approach to the problem could be a good solution as fast algorithms have been developed to minimise computation time. An example of this is the Hidden Markov Model which works on a dynamic Bayesian network\footnote{For more information \url{http://en.wikipedia.org/wiki/Hidden_Markov_model}.}. Here attributes or metadata of the article can be 
%
%\todo[inline]{Alternative solutions: graph based (Hidden-Markov-Models) and clustering, spatial distance (references to ConQuest NilssonHaav.pdf and 10 -Concept Spaces.pdf) in n-dimensional space is fast.}
%\todo[inline]{formal logical approach will probably be beaten by stochastic solutions in speed, however they are imprecise. The happy medium is a combination of logical and stochastic approaches. Trade-off: imprecise stochastic/precise logic. To find the balance is very hard.}
%\subsection{Existing solutions}
%Personalisation of digital solutions becomes more common everyday and users demand personalised solution to accommodate their needs.